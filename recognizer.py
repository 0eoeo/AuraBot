import os
import torch
import numpy as np
import scipy.signal
from fastapi import FastAPI, BackgroundTasks, HTTPException, Request
from fastapi.responses import FileResponse
import tempfile
import whisper
from dotenv import load_dotenv
import base64
import asyncio
import concurrent.futures

from voice import create_voice_answer
from generate_answer import BotState

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
app = FastAPI()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Whisper
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("tiny", device=device)

# –ü—É–ª –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Ç—è–∂—ë–ª—ã—Ö –∑–∞–¥–∞—á
executor = concurrent.futures.ThreadPoolExecutor()

# –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
blocked_phrases = set([
    "–¥–∏–Ω–∞–º–∏—á–Ω–∞—è –º—É–∑—ã–∫–∞", "—Ä–µ–¥–∞–∫—Ç–æ—Ä —Å—É–±—Ç–∏—Ç—Ä–æ–≤", "—Å–∏–ª—å–Ω—ã–π —à—É–º",
    "–±–µ–∑ –∑–≤—É–∫–∞", "–º—É–∑—ã–∫–∞–ª—å–Ω–∞—è –∑–∞—Å—Ç–∞–≤–∫–∞", "–∞—Ö –∞—Ö –∞—Ö",
    "–∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã", "—É—Ö —É—Ö —É—Ö", "—Ö–∞ —Ö–∞ —Ö–∞", "—Å–º–µ—Ö"
])

# –ö–æ–Ω—Ç–µ–∫—Å—Ç GigaChat
giga_chat_context = BotState()

def decode_speaker_name(encoded_name: str) -> str:
    try:
        return base64.b64decode(encoded_name).decode("utf-8")
    except Exception:
        return "–ë—Ä–æ"

def cleanup(paths):
    for path in paths:
        try:
            if path and os.path.exists(path):
                os.remove(path)
                print(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω —Ñ–∞–π–ª: {path}")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {path}: {e}")

async def transcribe_audio(model, audio_np: np.ndarray):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, lambda: model.transcribe(audio_np, language="ru"))

def preprocess_audio(audio_data: bytes) -> np.ndarray:
    try:
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º int16 PCM -> float32
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–µ—Ä–µ–æ/–º–æ–Ω–æ
        if len(audio_np) % 2 == 0:
            audio_np = audio_np.reshape(-1, 2).mean(axis=1)  # –°—Ç–µ—Ä–µ–æ -> –ú–æ–Ω–æ
        else:
            print("‚ö†Ô∏è –ê—É–¥–∏–æ –¥–ª–∏–Ω–∞ –Ω–µ –∫—Ä–∞—Ç–Ω–∞ 2 ‚Äî –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –º–æ–Ω–æ")

        # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ 48000 Hz -> 16000 Hz
        audio_np = scipy.signal.resample_poly(audio_np, up=1, down=3)

        return audio_np
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∞—É–¥–∏–æ: {e}")
        raise

@app.post("/recognize")
async def recognize(request: Request, background_tasks: BackgroundTasks):
    speaker_b64 = request.headers.get("X-Speaker-Name")
    speaker = decode_speaker_name(speaker_b64) if speaker_b64 else "–ë—Ä–æ"

    print(f"üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ—Ç {speaker}")

    audio_data = await request.body()
    if not audio_data:
        raise HTTPException(status_code=400, detail="No audio data provided")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö (~0.5 —Å–µ–∫—É–Ω–¥—ã)
    min_pcm_bytes = int(48000 * 2 * 2 * 0.5)  # 48000 samples/sec * 2 bytes/sample * 2 channels * 0.5 sec
    if len(audio_data) < min_pcm_bytes:
        print("‚ö†Ô∏è –ê—É–¥–∏–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return '', 204

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ
    try:
        audio_np = preprocess_audio(audio_data)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: {e}")
        raise HTTPException(status_code=400, detail="–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ")

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
    try:
        result = await transcribe_audio(model, audio_np)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏")

    recognized_text = result.get('text', '').strip()
    if not recognized_text:
        print("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø—É—Å—Ç–æ–π.")
        return '', 204

    full_text = f"[{speaker}]: {recognized_text}"
    print(f"üìù {full_text}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫-—Ñ—Ä–∞–∑—ã
    lower_text = full_text.lower()
    if any(phrase in lower_text for phrase in blocked_phrases):
        print("üö´ –ù–∞–π–¥–µ–Ω–∞ –±–ª–æ–∫-—Ñ—Ä–∞–∑–∞. –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –æ—Ç–≤–µ—Ç –Ω–µ –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")
        return '', 204

    if "–∑–∞–Ω–∏" not in lower_text:
        print("üîé –û–±—Ä–∞—â–µ–Ω–∏–µ '–ó–∞–Ω–∏' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Ç–≤–µ—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return '', 204

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    giga_chat_context.append_context(full_text)

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç GigaChat
    response_text = giga_chat_context.get_response_text()
    if not response_text:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –±–æ—Ç–∞")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    output_path = await create_voice_answer(response_text)

    if output_path:
        background_tasks.add_task(cleanup, [output_path])
        return FileResponse(output_path, media_type="audio/wav", filename="response.wav")
    else:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ –æ—Ç–≤–µ—Ç–∞")
