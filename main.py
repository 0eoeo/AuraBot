import base64

import numpy as np
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from generate_answer import BotState
from voice import create_voice_answer_stream
import torch
import whisper

app = FastAPI()
bot_state = BotState()
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("tiny", device=device)

# üîí –§—Ä–∞–∑—ã, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –æ—Ç–≤–µ—Ç –ù–ï –¥–æ–ª–∂–µ–Ω –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è
BLOCKED_PHRASES = [
    "–¥–∏–Ω–∞–º–∏—á–Ω–∞—è –º—É–∑—ã–∫–∞",
    "—Å–º–µ—Ö",
    "–≤–∫–ª—é—á–∏ –º—É–∑—ã–∫—É",
    "—Å—ã–≥—Ä–∞–π –ø–µ—Å–Ω—é",
    "–∞—Ö –∞—Ö –∞—Ö",
    "—É—Ö —É—Ö —É—Ö",
    "—Å–ø–æ–∫–æ–π–Ω–∞—è –º—É–∑—ã–∫–∞",
    "—Ä–µ–¥–∞–∫—Ç–æ—Ä —Å—É–±—Ç–∏—Ç—Ä–æ–≤"
]

# ‚úÖ –§—Ä–∞–∑—ã, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ–∫—Å—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞
ALLOWED_PHRASES = [
    "–≥–µ—Ä—Ç–∞",
    "?"
]

class AudioRequest(BaseModel):
    audio: list[float]

def decode_speaker_name(encoded_name: str) -> str:
    try:
        return base64.b64decode(encoded_name).decode("utf-8")
    except Exception:
        return "–ë—Ä–æ"

@app.post("/recognize")
async def recognize(request: Request, audio_data: AudioRequest):
    speaker_b64 = request.headers.get("X-Speaker-Name")
    speaker = decode_speaker_name(speaker_b64) if speaker_b64 else "–ë—Ä–æ"

    audio_np = np.array(audio_data.audio, dtype=np.float32)

    result = model.transcribe(audio_np, language="ru")
    text = result.get("text", "").strip().lower()
    print(f"üé§ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç {speaker}: {text}")

    # –ë–ª–æ–∫–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
    if any(phrase in text for phrase in BLOCKED_PHRASES):
        print("üö´ –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É.")
        return StreamingResponse(iter([]), media_type="audio/wav")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∞—è —Ñ—Ä–∞–∑–∞
    if any(phrase in text for phrase in ALLOWED_PHRASES):
        await bot_state.append_context(f"{speaker}: {text}")
        response_text = await bot_state.get_response_text()

        if response_text:
            return StreamingResponse(
                create_voice_answer_stream(response_text),
                media_type="audio/wav"
            )

    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—Ç—å
    return StreamingResponse(iter([]), media_type="audio/wav")
